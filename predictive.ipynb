{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predictive.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhumanGamer15/Predictive/blob/master/predictive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSpykVeaERcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3q3wlYBFS2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "isear=pd.read_excel(r'corona.xlsx')\n",
        "ttet=isear['SIT']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TNpN3_qjRLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text in ttet: \n",
        "  from spacy.lang.en import English\n",
        "  nlp=English()\n",
        "  my_doc=nlp(text)\n",
        "  filtered_sentence=[]\n",
        "  for token in my_doc:  \n",
        "    if(token.is_stop==False):  #remove stop words\n",
        "      if(token.is_punct==False):  #remove punctuations\n",
        "        filtered_sentence.append(token.lemma_.strip().lower()) #lemmatize and convert to lowercase\n",
        "  from collections import Counter  \n",
        "  word_freq=Counter(filtered_sentence)\n",
        "  common_words = word_freq.most_common(3)\n",
        "  unique_words = [word for (word, freq) in word_freq.items() if freq == 1] #unique words\n",
        "  nouns=[]\n",
        "  adjectives=[]\n",
        "  verb=[]\n",
        "  for token in unique_words: #parts of speech tagging\n",
        "    if token.pos_ == 'NOUN':\n",
        "      nouns.append(token)\n",
        "    elif token.pos_ == 'ADJ':\n",
        "      adjectives.append(token)\n",
        "    elif token.pos_ =='VERB':\n",
        "      verb.append(token)\n",
        "  from sklearn.feature_extraction.text import CountVectorizer #$vectorizerization\n",
        "  count_vect = CountVectorizer(analyzer=clean_text)\n",
        "  X_counts1 = count_vect.fit_transform(unique['body_text'])\n",
        "  from sklearn.feature_extraction.text import CountVectorizer\n",
        "  ngram_vect = CountVectorizer(ngram_range=(2,2),analyzer=clean_text) # It applies only bigram vectorizer\n",
        "  X_counts2 = ngram_vect.fit_transform(unique['body_text'])\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "  tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
        "  X_tfidf = tfidf_vect.fit_transform(unique['body_text'])\n",
        "  TRAIN_DATA = [\n",
        "        (\"Uber blew through $1 million a week\", {\"entities\": [(0, 4, \"ORG\")]}),\n",
        "        (\"Google rebrands its business apps\", {\"entities\": [(0, 6, \"ORG\")]})]\n",
        "\n",
        "  nlp = spacy.blank(\"en\")\n",
        "  optimizer = nlp.begin_training() #train data\n",
        "  for i in range(20):\n",
        "      random.shuffle(TRAIN_DATA)\n",
        "     for text, annotations in TRAIN_DATA:\n",
        "          nlp.update([text], [annotations], sgd=optimizer)\n",
        "  nlp.to_disk(\"/model\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYlRb0MKzvho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}